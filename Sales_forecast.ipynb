{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-18T18:23:31.846672Z",
     "start_time": "2025-09-18T18:23:30.450567Z"
    }
   },
   "source": [
    "# Import all required libraries at the beginning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For modeling\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('fivethirtyeight')"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatetime\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datetime\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'seaborn'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "stores_data = pd.read_csv('stores.csv')\n",
    "features_data = pd.read_csv('features.csv')\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Stores Data Shape:\", stores_data.shape)\n",
    "print(\"Features Data Shape:\", features_data.shape)\n",
    "\n",
    "print(\"\\nFirst few rows of Train Data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nFirst few rows of Stores Data:\")\n",
    "print(stores_data.head())\n",
    "\n",
    "print(\"\\nFirst few rows of Features Data:\")\n",
    "print(features_data.head())"
   ],
   "id": "ca3aee10377ba9fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge all datasets into one\n",
    "print(\"Merging datasets...\")\n",
    "\n",
    "# First merge train data with features\n",
    "merged_data = pd.merge(train_data, features_data, on=['Store', 'Date'], how='left')\n",
    "\n",
    "# Then merge with stores data\n",
    "merged_data = pd.merge(merged_data, stores_data, on=['Store'], how='left')\n",
    "\n",
    "# Convert Date to datetime\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in merged dataset:\")\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "# Display the merged dataset\n",
    "print(\"\\nMerged dataset shape:\", merged_data.shape)\n",
    "print(\"\\nFirst few rows of merged dataset:\")\n",
    "print(merged_data.head())"
   ],
   "id": "a4b9bfb6092ba27a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Handle missing values - fill with appropriate values\n",
    "# For MarkDown columns, fill with 0 assuming no markdown when missing\n",
    "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "for col in markdown_cols:\n",
    "    merged_data[col].fillna(0, inplace=True)\n",
    "\n",
    "# For CPI and Unemployment, forward fill then backward fill\n",
    "merged_data['CPI'].fillna(method='ffill', inplace=True)\n",
    "merged_data['CPI'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "merged_data['Unemployment'].fillna(method='ffill', inplace=True)\n",
    "merged_data['Unemployment'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Check if all missing values are handled\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "# Create additional date-related features\n",
    "merged_data['Year'] = merged_data['Date'].dt.year\n",
    "merged_data['Month'] = merged_data['Date'].dt.month\n",
    "merged_data['Week'] = merged_data['Date'].dt.isocalendar().week\n",
    "merged_data['Day'] = merged_data['Date'].dt.day\n",
    "\n",
    "# Create holiday flag\n",
    "merged_data['IsHoliday'] = merged_data['IsHoliday'].astype(int)\n",
    "\n",
    "print(\"\\nDataset after preprocessing:\")\n",
    "print(merged_data.head())"
   ],
   "id": "ddb741fa15dd85df",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
